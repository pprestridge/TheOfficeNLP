{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1be2ffe3",
   "metadata": {},
   "source": [
    "# NLP The Office\n",
    "\n",
    "Training an NLP model on transcripts of episodes of the office to classify the speaker as either Jim or Dwight. Then, a second NLP model will be used to generate lines for Michael\n",
    "\n",
    "Steps:\n",
    "\n",
    "    - WebScrape all episodes of the office\n",
    "    \n",
    "    - Use TensorFlow to transform lines into padded sequences amnenable for a Nueral Net\n",
    "    \n",
    "    - Train a Classification model to determine the speaker based on a line\n",
    "    \n",
    "    - Transform all of Michael's lines for a speech generation model\n",
    "    \n",
    "    - Train a prediction model to generate Michael's lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646d0ed5",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05077531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib3\n",
    "import shutils\n",
    "import tqdm\n",
    "import random\n",
    "import PIL\n",
    "import wget\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "padder = tf.keras.preprocessing.sequence.pad_sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ed9bec",
   "metadata": {},
   "source": [
    "# Scrape Data\n",
    "\n",
    "In order to train an NLP model, we first need to develop our corpus of text. For that, we will parse through scripts looking for lines from Jim and Dwight. The individual office transcripts are listed across 7 different webapages. The first function urlFinder will be used to generate a url for each individual script from the 7 webpages. The second function buildCorpus will actually build the corpus of text for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e744db50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def urlFinder(urls):\n",
    "    \n",
    "    # Initiate list of links\n",
    "    urlList = []\n",
    "    base = 'https://transcripts.foreverdreaming.org'\n",
    "    \n",
    "    for url in urls:\n",
    "        \n",
    "        # Access given url \n",
    "        webpage = requests.get(url)\n",
    "    \n",
    "        # Use Beautiful Soup to parse through the webpage and make it accesible\n",
    "        soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "        \n",
    "        # Find ever link associated with an episode\n",
    "        count = 0\n",
    "        for entry in soup.find_all(\"a\", class_='topictitle'):\n",
    "        \n",
    "            # Find the links\n",
    "            link = entry[\"href\"]\n",
    "    \n",
    "            # Add base the of link back in and store it\n",
    "            link = base + link[1:]\n",
    "            urlList.append(link)\n",
    "            \n",
    "            # Remove first link on each webpage\n",
    "            if count == 0:\n",
    "                urlList.pop()\n",
    "            count+=1\n",
    "    \n",
    "    return urlList\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4ebd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of homepage URLs that link to individual transcript links\n",
    "urls =[\"https://transcripts.foreverdreaming.org/viewforum.php?f=574\",\n",
    "       \"https://transcripts.foreverdreaming.org/viewforum.php?f=574&start=25\",\n",
    "       \"https://transcripts.foreverdreaming.org/viewforum.php?f=574&start=50\",\n",
    "       \"https://transcripts.foreverdreaming.org/viewforum.php?f=574&start=75\",\n",
    "       \"https://transcripts.foreverdreaming.org/viewforum.php?f=574&start=100\",\n",
    "       \"https://transcripts.foreverdreaming.org/viewforum.php?f=574&start=125\",\n",
    "       \"https://transcripts.foreverdreaming.org/viewforum.php?f=574&start=150\"]\n",
    "\n",
    "# From the homepage, find the URL for each individual scripts\n",
    "urlList = urlFinder(urls)\n",
    "\n",
    "print(len(urlList))\n",
    "print(urlList[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908391ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildCorpus(urlList, characters):\n",
    "    \n",
    "    # Initiate a list of lines and characters to pull lines for\n",
    "    lines = []\n",
    "    \n",
    "    for url in urlList:\n",
    "    # Get all the text for a given url\n",
    "        webpage = requests.get(url)\n",
    "        soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "        text = soup.text\n",
    "    \n",
    "        # Initialize the while loop by finding the first instance of a labeled character speaking\n",
    "        startInd = indexFinder(text, characters)\n",
    "        moreLines = True\n",
    "    \n",
    "        # Work through the script until there isn't anymore speach from characters\n",
    "        while moreLines == True:\n",
    "        \n",
    "            # Select the portion of the string where a labeled character is speaking\n",
    "            text = text[startInd:]\n",
    "            stopInd = text.find(\"\\n\")\n",
    "            passage = text[:stopInd]\n",
    "            \n",
    "            # Remove action dialogue -- Keeping for now\n",
    "            #passage = actionRemover(passage)\n",
    "            \n",
    "            # Split the passage into a label and sentence\n",
    "            labeledPasssage = passage.split(\":\")\n",
    "            lines.append(labeledPasssage)\n",
    "\n",
    "\n",
    "            # Find next occurence of a labeled speaker\n",
    "            text = text[stopInd:]\n",
    "            startInd = indexFinder(text, characters)\n",
    "    \n",
    "            # If the helper function doens't find another location of a chracter speaking it will return 1e10\n",
    "            if startInd == -1:\n",
    "                moreLines = False\n",
    "    \n",
    "    return lines\n",
    "\n",
    "\n",
    "# Indentify the next time a labeled character speaks\n",
    "def indexFinder(text, characters):\n",
    "\n",
    "    # Initialize an impossible index\n",
    "    index = 1e10\n",
    "    \n",
    "    # Cycle through a list of characters\n",
    "    for character in characters:\n",
    "        \n",
    "        # Find the first index, but return a -1, when there isn't an existing instance\n",
    "        try:\n",
    "            ind = text.index(character)\n",
    "        except ValueError:\n",
    "            ind = -1\n",
    "        \n",
    "        # Compare index against all existing values\n",
    "        if ind < index:\n",
    "            index = ind\n",
    "    \n",
    "    return index\n",
    "\n",
    "def actionRemover(passage):\n",
    "    \n",
    "    # Look for any instances of action dialogue\n",
    "    stillAction = True\n",
    "    try:\n",
    "        ind1 = passage.index(\"[\")\n",
    "    except ValueError:\n",
    "        stillAction = False\n",
    "    \n",
    "    \n",
    "    while stillAction:\n",
    "\n",
    "        ind2 = passage.index(\"]\")\n",
    "        passage = passage.replace(passage[ind1:ind2], \" \")\n",
    "\n",
    "\n",
    "        try:\n",
    "            ind = passage.index(\"[\")\n",
    "        except ValueError:\n",
    "            stillAction = False\n",
    "        \n",
    "    return passage\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b89500d",
   "metadata": {},
   "source": [
    "## Save / Load Data\n",
    "\n",
    "Now that the corpus has been built, let's store it as a pickled file so that it can be loaded again quickly without having to scrape it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33af2ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = [\"Jim:\", \"Dwight:\"]\n",
    "lines = buildCorpus(urlList, characters)\n",
    "\n",
    "print(lines[3])\n",
    "print(lines[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aee7e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('labeledData.txt', 'wb') as fh:\n",
    "    pickle.dump(lines, fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a6bc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickledfile = open('labeledData.txt', 'rb')\n",
    "lines = pickle.load(pickledfile)\n",
    "\n",
    "print(lines[3])\n",
    "print(lines[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e9f7c4",
   "metadata": {},
   "source": [
    "## Train and Test splitting\n",
    "\n",
    "Splitting the data into a test and train arrays for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8966e933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Define the test and training split\n",
    "test_train_split = round(len(lines) * 0.1)\n",
    "\n",
    "# Shuffle and assign the data\n",
    "lines = random.sample(lines, len(lines))\n",
    "train_lines = lines[test_train_split:]\n",
    "test_lines = lines[:test_train_split]\n",
    "\n",
    "\n",
    "train_sentences = []\n",
    "train_labels = []\n",
    "for i in range(len(train_lines)):\n",
    "    try:\n",
    "        train_sentences.append(train_lines[i][1])\n",
    "        train_labels.append(train_lines[i][0])\n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "test_sentences = []\n",
    "test_labels = []\n",
    "for i in range(len(test_lines)):\n",
    "    try:\n",
    "        test_sentences.append(test_lines[i][1])\n",
    "        test_labels.append(test_lines[i][0])\n",
    "    except IndexError:\n",
    "        pass    \n",
    "\n",
    "print(train_labels[0])\n",
    "print(train_sentences[0])\n",
    "\n",
    "print(len(train_sentences))\n",
    "print(len(test_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0acb45",
   "metadata": {},
   "source": [
    "## Preprocessing Inputs\n",
    "\n",
    "Before training, the sentences/lines needs to be processed before being fed to the NLP model. We will use the Tokenizer and pad_sequences from tensorflow to create tokenized sequences of equal lenghts. The speaker label will also be tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe00d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer\n",
    "max_length = 40\n",
    "vocab = 80000\n",
    "oov_token = \"<OOV>\"\n",
    "embedding_dim = 100\n",
    "padding = \"post\"\n",
    "\n",
    "padder = tf.keras.preprocessing.sequence.pad_sequences\n",
    "textTokenizer = tf.keras.preprocessing.text.Tokenizer(num_words = vocab, oov_token=oov_token)\n",
    "\n",
    "# Process training inputs\n",
    "textTokenizer.fit_on_texts(train_sentences)\n",
    "train_sequences = textTokenizer.texts_to_sequences(train_sentences)\n",
    "train_sequences = np.array(padder(train_sequences, padding=\"post\", maxlen=max_length, truncating=\"post\"))\n",
    "\n",
    "# Process test inputs\n",
    "test_sequences = textTokenizer.texts_to_sequences(test_sentences)\n",
    "test_sequences = np.array(padder(test_sequences, padding=\"post\", maxlen=max_length, truncating=\"post\"))\n",
    "\n",
    "# Process lables\n",
    "labelTokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "labelTokenizer.fit_on_texts(train_labels)\n",
    "train_labels_seq = np.array(labelTokenizer.texts_to_sequences(train_labels))\n",
    "test_labels_seq = np.array(labelTokenizer.texts_to_sequences(test_labels))\n",
    "\n",
    "train_labels_enc = tf.keras.utils.to_categorical(train_labels_seq)\n",
    "train_labels_enc = train_labels_enc[:,1]\n",
    "test_labels_enc = tf.keras.utils.to_categorical(test_labels_seq)\n",
    "test_labels_enc = test_labels_enc[:,1]\n",
    "\n",
    "print(train_sequences[1])\n",
    "print(train_labels_seq[1])\n",
    "                       \n",
    "print(train_sequences.shape)\n",
    "print(train_labels_enc.shape)\n",
    "print(test_sequences.shape)\n",
    "print(test_labels_enc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bc5376",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "A lot of lines tend to be on the order of 5 or so words, which is challenging for a classifier model to handle. Although there are dead-giveaway words like when Jim calls Dwight by his name or vice-versa, 5 words isn't enough information to accuratley classify a character. shortSentenceRemover will handle removing all sentences that are less than a specified length. Although this greatly reduces the training data, the quality greatly increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f763fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortSentenceRemover(labels, sequences, sentenceLength):\n",
    "    \n",
    "    # Run though each entry and check the minimum setenceLength criteria is met\n",
    "    count = 0\n",
    "    while count < len(sequences):\n",
    "            \n",
    "            #Check if the minLength word space on the count row is filled in, indicating there are at least minLegth words\n",
    "            if sequences[count, sentenceLength] == 0:\n",
    "                \n",
    "                #Delete row and don't progress count\n",
    "                sequences = np.delete(sequences, (count), axis=0)\n",
    "                labels = np.delete(labels, (count), axis=0)\n",
    "                \n",
    "            else:\n",
    "                count += 1\n",
    "    \n",
    "    return labels, sequences \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cced9966",
   "metadata": {},
   "outputs": [],
   "source": [
    "minLength = 5\n",
    "train_labels_enc, train_sequences = shortSentenceRemover(train_labels_enc, train_sequences, minLength)\n",
    "test_labels_enc, test_sequences = shortSentenceRemover(test_labels_enc, test_sequences, minLength)\n",
    "\n",
    "print(train_sequences.shape)\n",
    "print(train_labels_enc.shape)\n",
    "print(test_sequences.shape)\n",
    "print(test_labels_enc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feccfef7",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "\n",
    "For our classifier, we will use a relatively simple NLP model. I wanted to use the Standford Glove embedding weights, but the webiste wouldn't respond when trying to download the weights. Instead, the embedding layer will train itself. Followed by the embedding layer is a Bidirectional LSTM to handle the transfer of meaning forward and backwards in the sentence. Lastly, two dense layers connect to the output. I have the code written such that the model can operate as a binary and multiclass classifier. If the cell containing the characters list is updated to include more characters, the notebook and model will update the preprocessing and output layer of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3721aa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "XavierInit = tf.keras.initializers.GlorotNormal()\n",
    "\n",
    "def buildModel(max_length, embedding_dim, vocab, characters):\n",
    "    \n",
    "    #Embedding Layer\n",
    "    sequence = tf.keras.layers.Input(shape = (max_length))\n",
    "    x = tf.keras.layers.Embedding(vocab+1, embedding_dim, input_length=max_length)(sequence)\n",
    "    \n",
    "    # Bidirectional LSTM\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True, kernel_initializer=XavierInit))(x)\n",
    "    \n",
    "    # Conv1D\n",
    "    x = tf.keras.layers.Conv1D(128, kernel_size = 3 ,kernel_initializer=XavierInit)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "    # Dense Layers\n",
    "    x = tf.keras.layers.Dense(64, activation = 'relu', kernel_initializer=XavierInit)(x)\n",
    "    x = tf.keras.layers.Dropout(0.9)(x)\n",
    "    x = tf.keras.layers.Dense(64, activation = 'relu', kernel_initializer=XavierInit)(x)\n",
    "    x = tf.keras.layers.Dropout(0.9)(x)\n",
    "\n",
    "\n",
    "    # Output Layer\n",
    "    x = tf.keras.layers.Dense(1, activation = 'sigmoid')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs = sequence, outputs=x)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecb7eac",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = buildModel(max_length, embedding_dim, vocab, characters)\n",
    "Adam = tf.keras.optimizers.Adam\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.00009), metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4a0eb7",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef8854d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup callback to stop training if accuracy reaches 95%\n",
    "class mycallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs = {}):\n",
    "        if (logs.get('accuracy') > 0.95):\n",
    "            model.self.training = False\n",
    "            \n",
    "acc_Callback = mycallback()\n",
    "\n",
    "# Setup callback to stop training if validation loss stops improving (overfitting sign)\n",
    "earlyStop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 15, restore_best_weights = True)\n",
    "\n",
    "history = model.fit(train_sequences, train_labels_enc, epochs = 500, \n",
    "                    batch_size = 128, steps_per_epoch = 16,\n",
    "                    shuffle = True,\n",
    "                    validation_data=(test_sequences, test_labels_enc),\n",
    "                    callbacks = [acc_Callback, earlyStop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99445237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "epochs = [*range(1,len(acc)+1)]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(epochs, acc)\n",
    "ax.plot(epochs,val_accuracy)\n",
    "fig.legend([\"Accuracy\" ,\"Validation Accuracy\"])\n",
    "ax.set(xlabel=\"Epochs\", ylabel=\"Percent Correct\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(epochs, loss)\n",
    "ax.plot(epochs,val_loss)\n",
    "fig.legend([\"Loss\" ,\"Validation Loss\"])\n",
    "ax.set(xlabel=\"Epochs\", ylabel=\"Loss Function\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d44967",
   "metadata": {},
   "source": [
    "# Line Generation\n",
    "\n",
    "Now we will build a model to take a given sequence and predict the next word. When called continuously the model will generate a setence. To ensure there is enough data, we will train the model on Michael, and all of of the webscraping/data preprocessing from before will be helpful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a786fdd",
   "metadata": {},
   "source": [
    "## WebScraping\n",
    "\n",
    "Pull all of Michael's lines and save them to a pickeled file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cf777a",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = [\"Michael:\"]\n",
    "lines = buildCorpus(urlList, characters)\n",
    "\n",
    "print(lines[3])\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('Michael.txt', 'wb') as fh:\n",
    "    pickle.dump(lines, fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd16dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickledfile = open('Michael.txt', 'rb')\n",
    "label_lines = pickle.load(pickledfile)\n",
    "lines[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df41b553",
   "metadata": {},
   "source": [
    "## Input Data Structuring\n",
    "\n",
    "To train this model. All of Michael's lines need to be broken down to include the preceding words as inputs and next word as the label. For example, let's take his infamous line: “If I had a gun with two bullets and I was in a room with Hitler, Bin Laden, and Toby, I would shoot Toby twice.”\n",
    "\n",
    "This sentence could then be broken up into a number of training examples:\n",
    "\n",
    "        Input: \"If\"          Output = \"I\"\n",
    "        \n",
    "        Input: \"If I\"        Output = \"had\"\n",
    "        \n",
    "        Input: \"If I had\"    Output = \"a\"\n",
    "        \n",
    "        Input: \"If I had a\"  Output = \"gun\"\n",
    "\n",
    "        etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2541e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the existing labels\n",
    "lines = []\n",
    "for i in range(len(label_lines)):\n",
    "    try:\n",
    "        lines.append(label_lines[i][1])\n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "# Initialize the tokenizer\n",
    "max_length = 20\n",
    "vocab = 5000\n",
    "oov_token = \"<OOV>\"\n",
    "embedding_dim = 40\n",
    "padding = \"post\"\n",
    "mikeTokenizer = tf.keras.preprocessing.text.Tokenizer(num_words = vocab, oov_token=oov_token)\n",
    "\n",
    "# Tokenize the sentences\n",
    "mikeTokenizer.fit_on_texts(lines)\n",
    "mikeSequences = mikeTokenizer.texts_to_sequences(lines)\n",
    "\n",
    "\n",
    "# Break them up into preceding words and labels\n",
    "input_sequences = []\n",
    "for line in mikeSequences:\n",
    "    for i in range(1, len(line)):\n",
    "        n_sequence = line[:i+1]\n",
    "        input_sequences.append(n_sequence)\n",
    "\n",
    "# Pad inputs\n",
    "input_sequences = np.array(padder(input_sequences, padding=\"pre\", maxlen=max_length+1, truncating=\"pre\"))\n",
    "\n",
    "# Sort sequences into inputs and labels\n",
    "inputs = input_sequences[:,:-1]\n",
    "labels = input_sequences[:,-1]\n",
    "\n",
    "# Check sizing and scripting are working correclty\n",
    "print(inputs[0:3])\n",
    "print(labels[0:3])\n",
    "print(inputs.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113ec258",
   "metadata": {},
   "source": [
    "## Model Creation & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1bc4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildInferenceModel(vocab, max_length):\n",
    "\n",
    "    # Input Embedding layer\n",
    "    sequence = tf.keras.layers.Input(shape = (max_length))\n",
    "    inputs = tf.keras.layers.Embedding(vocab+1, 100, input_length=max_length)(sequence)\n",
    "    \n",
    "    # Bidirectional LSTM\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences = True))(inputs)\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences = True))(x)\n",
    "    x = tf.keras.layers.Conv1D(128, kernel_size=5)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(512, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.Dense(512, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    \n",
    "    # Output\n",
    "    output = tf.keras.layers.Dense(vocab+1, activation = \"softmax\")(x)\n",
    "    \n",
    "    # Create & return model\n",
    "    model = tf.keras.Model(sequence,output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcadaf8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mikeModel = buildInferenceModel(vocab, max_length)\n",
    "Adam = tf.keras.optimizers.Adam\n",
    "mikeModel.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(lr=0.00001), metrics=['accuracy'])\n",
    "\n",
    "mikeModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47cf5d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup callback to stop training if validation loss stops improving (overfitting sign)\n",
    "earlyStop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 15, restore_best_weights = True)\n",
    "\n",
    "history = mikeModel.fit(inputs, labels, \n",
    "                        shuffle = True, epochs=500, \n",
    "                        validation_split =  0.05, callbacks = [earlyStop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3d909b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961d6dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "epochs = [*range(1,len(acc)+1)]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(epochs, acc)\n",
    "ax.plot(epochs,val_accuracy)\n",
    "fig.legend([\"Accuracy\" ,\"Validation Accuracy\"])\n",
    "ax.set(xlabel=\"Epochs\", ylabel=\"Percent Correct\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(epochs, loss)\n",
    "ax.plot(epochs,val_loss)\n",
    "fig.legend([\"Loss\" ,\"Validation Loss\"])\n",
    "ax.set(xlabel=\"Epochs\", ylabel=\"Loss Function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90dc57e",
   "metadata": {},
   "source": [
    "## Line Generation\n",
    "\n",
    "With a NLP model that generates the next word in a sequence, we can start a line and let the model finish it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14296beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mikeTalk(start, model, tokenizer, num_words):\n",
    "    \n",
    "    #Format input string into something the model can evalutate\n",
    "    Sequence = tokenizer.texts_to_sequences(start)\n",
    "    \n",
    "    #Cycle the function as generate to the desired word length\n",
    "    for i in range(num_words):\n",
    "        \n",
    "        #Handle input and truncation\n",
    "        Sequence_input = np.array(padder(Sequence, padding=\"pre\", maxlen=max_length, truncating = \"pre\")).reshape(1,max_length)\n",
    "    \n",
    "        #Predict word\n",
    "        word_liklihood = model.predict(Sequence_input)\n",
    "        \n",
    "        #Remove OOV Token from selected vocabulary\n",
    "        word_liklihood[-1] = -1\n",
    "\n",
    "        #Pick word with the highest likelihood\n",
    "        word = np.argmax(word_liklihood)\n",
    "        \n",
    "        #Append word\n",
    "        Sequence[0].append(word)\n",
    "\n",
    "\n",
    "    #Return and print the input and output for a line\n",
    "    print(\"Input: \")\n",
    "    print(start)\n",
    "    print(\"\\n\")\n",
    "    print(\"Output: \")\n",
    "    print(tokenizer.sequences_to_texts(Sequence))\n",
    "    \n",
    "    return Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faefbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 4\n",
    "line = mikeTalk([\"Dwight and Jim\"], mikeModel, mikeTokenizer, num_words)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
